import{_ as a,o as c,c as t,b as s}from"./index-5ac8332a.js";const e={},l=s(`<h1 id="llama-terminal-completion" data-v-886cc147>Llama Terminal Completion</h1><p data-v-886cc147>Ever wish you could look up Linux commands or ask questions and receive responses from the terminal? You probably need a paid service, an API key with paid usage, or at least an internet connection, right? Not with Llama Terminal Completion. Instead, we&#39;ll Run a Large Language Model (think ChatGPT) locally, on your personal machine, and generate responses from there.</p><h2 id="table-of-contents" data-v-886cc147>Table of Contents</h2><ul data-v-886cc147><li data-v-886cc147><a href="#llama-terminal-completion" data-v-886cc147>Llama Terminal Completion</a><ul data-v-886cc147><li data-v-886cc147><a href="#table-of-contents" data-v-886cc147>Table of Contents</a></li><li data-v-886cc147><a href="#installation" data-v-886cc147>Installation</a><ul data-v-886cc147><li data-v-886cc147><a href="#llama-cpp-installation" data-v-886cc147>Llama.cpp installation</a></li><li data-v-886cc147><a href="#llama-terminal-completion-installation" data-v-886cc147>Llama Terminal Completion installation</a></li><li data-v-886cc147><a href="#environment-variables" data-v-886cc147>Environment Variables</a></li></ul></li><li data-v-886cc147><a href="#usage" data-v-886cc147>Usage</a><ul data-v-886cc147><li data-v-886cc147><a href="#alias" data-v-886cc147>Alias</a></li></ul></li><li data-v-886cc147><a href="#contributing" data-v-886cc147>Contributing</a></li><li data-v-886cc147><a href="#license" data-v-886cc147>License</a></li></ul></li></ul><p data-v-886cc147>This Python script interacts with the <a href="https://github.com/ggerganov/llama.cpp" data-v-886cc147>llama.cpp</a> library to provide virtual assistant capabilities through the command line. It allows you to ask questions and receive intelligent responses, as well as generate Linux commands based on your prompts.</p><h2 id="installation" data-v-886cc147>Installation</h2><h3 id="llama-cpp-installation" data-v-886cc147>Llama.cpp installation</h3><ol data-v-886cc147><li data-v-886cc147>Clone the &#39;llama.cpp&#39; repository to your local machine <pre data-v-886cc147><code class="lang-bash" data-v-886cc147>git <span class="hljs-keyword" data-v-886cc147>clone</span> <span class="hljs-title" data-v-886cc147>https</span>://github.com/ggerganov/llamacpp.git
</code></pre></li><li data-v-886cc147>Build the llama.cpp library by following the instructions in the llama.cpp repository. A good tutorial for this can be found at <a href="https://wandb.ai/capecape/LLMs/reports/How-to-Run-LLMs-Locally--Vmlldzo0Njg5NzMx" data-v-886cc147>How to Run LLMs Locally</a></li></ol><h3 id="llama-terminal-completion-installation" data-v-886cc147>Llama Terminal Completion installation</h3><ol data-v-886cc147><li data-v-886cc147>Clone the llama-terminal-completion repository to your local machine: <pre data-v-886cc147><code class="lang-bash" data-v-886cc147>git <span class="hljs-keyword" data-v-886cc147>clone</span> <span class="hljs-title" data-v-886cc147>https</span>://github.com/adammpkins/llama-terminal-completion.git
</code></pre></li><li data-v-886cc147>Set up the environment variables (see below)</li></ol><h3 id="environment-variables" data-v-886cc147>Environment Variables</h3><p data-v-886cc147>Before using this script, you need to set up the <code data-v-886cc147>LLAMA_COMPLETION_DIR</code> and <code data-v-886cc147>LLAMA_CPP_DIR</code> environment variables. These variables point to the directories where the <code data-v-886cc147>llama-terminal-completion</code> and <code data-v-886cc147>llama.cpp</code> files are located, respectively. You can set these variables in your shell configuration file (e.g., <code data-v-886cc147>.bashrc</code> or <code data-v-886cc147>.zshrc</code>) like this:</p><pre data-v-886cc147><code class="lang-bash" data-v-886cc147><span class="hljs-keyword" data-v-886cc147>export</span> LLAMA_COMPLETION_DIR=<span class="hljs-string" data-v-886cc147>&quot;/path/to/llama-terminal-completion/&quot;</span>
<span class="hljs-keyword" data-v-886cc147>export</span> LLAMA_CPP_DIR=<span class="hljs-string" data-v-886cc147>&quot;/path/to/llama.cpp/&quot;</span>
</code></pre><p data-v-886cc147>Replace /path/to/llama-terminal-completion/ and /path/to/llama.cpp/ with the actual paths to the respective directories on your system.</p><h2 id="usage" data-v-886cc147>Usage</h2><p data-v-886cc147>Open a terminal window.</p><p data-v-886cc147>Navigate to the directory where the ask_llama.py script is located.</p><p data-v-886cc147>Run the script with the desired options. Here are some examples:</p><ul data-v-886cc147><li data-v-886cc147>To generate a Linux command based on a prompt: <pre data-v-886cc147><code class="lang-bash" data-v-886cc147>  python3 ask_llama.py <span class="hljs-comment" data-v-886cc147>&quot;list the contents of the current directory&quot;</span>
</code></pre></li><li data-v-886cc147><p data-v-886cc147>To ask a question to the virtual assistant:</p><pre data-v-886cc147><code class="lang-bash" data-v-886cc147>  python3 ask_llama<span class="hljs-selector-class" data-v-886cc147>.py</span> -<span class="hljs-selector-tag" data-v-886cc147>q</span> <span class="hljs-string" data-v-886cc147>&quot;How does photosynthesis work?&quot;</span>
</code></pre></li><li data-v-886cc147><p data-v-886cc147>To clear the history of commands:</p><pre data-v-886cc147><code class="lang-bash" data-v-886cc147>  python3 ask_llama<span class="hljs-selector-class" data-v-886cc147>.py</span> -ch
</code></pre></li></ul><p data-v-886cc147>For more options, you can run:</p><pre data-v-886cc147><code class="lang-bash" data-v-886cc147><span class="hljs-keyword" data-v-886cc147>python3</span> ask_llama.<span class="hljs-keyword" data-v-886cc147>py</span> --<span class="hljs-keyword" data-v-886cc147>help</span>
</code></pre><p data-v-886cc147>Its output is as follows:</p><pre data-v-886cc147><code class="lang-bash" data-v-886cc147>Usage: python3 ask_llama.py [prompt]
Example: python3 ask_llama.py <span class="hljs-string" data-v-886cc147>&#39;list all files in the current directory&#39;</span>
Options:
-q                ask <span class="hljs-keyword" data-v-886cc147>a</span> question <span class="hljs-built_in" data-v-886cc147>to</span> <span class="hljs-keyword" data-v-886cc147>the</span> virtual assistant
-ch               <span class="hljs-built_in" data-v-886cc147>clear</span> <span class="hljs-keyword" data-v-886cc147>the</span> history <span class="hljs-keyword" data-v-886cc147>of</span> commands
-cqh              <span class="hljs-built_in" data-v-886cc147>clear</span> <span class="hljs-keyword" data-v-886cc147>the</span> history <span class="hljs-keyword" data-v-886cc147>of</span> questions
-h                show <span class="hljs-keyword" data-v-886cc147>the</span> history <span class="hljs-keyword" data-v-886cc147>of</span> commands
-qh               show <span class="hljs-keyword" data-v-886cc147>the</span> history <span class="hljs-keyword" data-v-886cc147>of</span> questions
-v                show <span class="hljs-keyword" data-v-886cc147>the</span> <span class="hljs-built_in" data-v-886cc147>version</span> <span class="hljs-keyword" data-v-886cc147>of</span> llama-terminal-completion
<span class="hljs-comment" data-v-886cc147>--help            show this help message and exit</span>
</code></pre><h3 id="alias" data-v-886cc147>Alias</h3><p data-v-886cc147>You can create an alias for the script in your shell configuration file (e.g., <code data-v-886cc147>.bashrc</code> or <code data-v-886cc147>.zshrc</code>) like this:</p><pre data-v-886cc147><code class="lang-bash" data-v-886cc147><span class="hljs-keyword" data-v-886cc147>alias</span> <span class="hljs-title" data-v-886cc147>ask</span>=<span class="hljs-string" data-v-886cc147>&quot;python3 /path/to/llama-terminal-completion/ask_llama.py&quot;</span>
</code></pre><p data-v-886cc147>Then you can run the script like this:</p><pre data-v-886cc147><code class="lang-bash" data-v-886cc147>ask <span class="hljs-comment" data-v-886cc147>&quot;list the contents of the current directory&quot;</span>
</code></pre><h2 id="contributing" data-v-886cc147>Contributing</h2><p data-v-886cc147>Contributions to this project are welcome! Feel free to fork the repository, make changes, and submit pull requests.</p><h2 id="license" data-v-886cc147>License</h2><p data-v-886cc147>This project is licensed under the <a href="https://choosealicense.com/licenses/mit/" data-v-886cc147>MIT License</a></p>`,32),n=[l];function o(i,p){return c(),t("main",null,n)}const r=a(e,[["render",o],["__scopeId","data-v-886cc147"]]);export{r as default};
